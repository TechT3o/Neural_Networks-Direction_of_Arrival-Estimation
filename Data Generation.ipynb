{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyroomacoustics as pra\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import webrtcvad\n",
    "import sounddevice as sd\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c360c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npow2 (x) :\n",
    "    # finds the next power of 2 of the input x\n",
    "    return 1 <<(x -1) . bit_length ()\n",
    "\n",
    "def stftprep (x , fs , time ):\n",
    "    # splits the input array in an array of frames of fixed length (to be used to prepare\n",
    "    #array for VAD or STFT )\n",
    "    \n",
    "    #x = input array\n",
    "    #fs = sampling frequency\n",
    "    # time = time of the split frames ( determines the frame length )\n",
    "\n",
    "    # returns the array of the split input data\n",
    "    frames = int( time * fs )\n",
    "    X = np.array([ x[i: i+ frames] for i in range(0, len(x)-frames, frames)])\n",
    "    return X\n",
    "\n",
    "def VAD(aggr , data ,sr , time ):\n",
    "    #This function uses the webrtcvad module to do voice activity of detection\n",
    "    #on a numpy array of audio data and returns the array of the voiced segments\n",
    "    \n",
    "    \n",
    "    # aggr how sensitive voice detection is with options 0(least),1,2,3(most)\n",
    "    # data ( numpy array ) to be classified as voiced / unvoiced\n",
    "    #sr is the sampling rate\n",
    "    # the time of the frame in ms (10 ,20 ,30 ms)\n",
    "\n",
    "    # Sets the vad object\n",
    "    vad = webrtcvad . Vad ( aggr )\n",
    "    # sets frame length\n",
    "    framesz = time /1000\n",
    "    # cuts the data in the frame size\n",
    "    Z = stftprep ( data , sr , time /1000)\n",
    "    # returns indexes of voiced data\n",
    "    ind = [i for i in range ( len (Z) ) if vad . is_speech ( np . int16 ( Z[i ]*32768) . tobytes () ,sr )]\n",
    "    # multiply by 32768\n",
    "    # creates the array of the voiced parts of the data\n",
    "    emin = [Z [k] for k in ind ]\n",
    "    eminor = np . array ([ j for i in emin for j in i ])\n",
    "    return eminor\n",
    "\n",
    "def gcc_phat ( sig , refsig , fs , max_tau = None , interp =4) :\n",
    "    '''\n",
    "    I modified this function from the original :\n",
    "    Copyright (c) 2017 Yihui Xiong\n",
    "    Licensed under the Apache License , Version 2.0 ( the \" License \");\n",
    "    you may not use this file except in compliance with the License .\n",
    "    You may obtain a copy of the License at\n",
    "    http :// www . apache . org/ licenses / LICENSE -2.0\n",
    "    '''\n",
    "    \n",
    "    #This function computes the offset between the signal sig and the reference signal refsig\n",
    "    #using the Generalized Cross Correlation - Phase Transform (GCC - PHAT ) method.\n",
    "\n",
    "    # make sure the length for the FFT is larger or equal than len ( sig ) + len ( refsig )\n",
    "    n = sig.shape[0] + refsig.shape[0]\n",
    "\n",
    "    # find next power of 2 to make FFT faster\n",
    "    n = npow2(n)\n",
    "\n",
    "    # Generalized Cross Correlation Phase Transform\n",
    "    # Uncomment to add window , multiply the signals sig and refsig with the window\n",
    "    # window = np. hamming ( len ( sig ))\n",
    "\n",
    "    SIG = np.fft.rfft(sig, n=n)\n",
    "    REFSIG = np.fft.rfft(refsig , n=n)\n",
    "\n",
    "    R = SIG * np.conj(REFSIG)\n",
    "    # phat weighting\n",
    "    ab = np.abs(R)\n",
    "    ab [ab<1e-10] = 1e-10\n",
    "    # obtains gcc\n",
    "    cc = np.fft.irfft(R/np.abs(ab),n=npow2(interp*n))\n",
    "\n",
    "    # Uncomment below to normalize\n",
    "    #cc = cc/np. max (cc)\n",
    "\n",
    "    max_shift = int(interp*n/2)\n",
    "    if max_tau:\n",
    "        max_shift = np.minimum(int(interp*fs*max_tau),max_shift)\n",
    "    \n",
    "    cc = np.concatenate((cc[-max_shift:], cc[:max_shift +1]) )\n",
    "    # find max cross correlation index ( TDOA )\n",
    "    shift = np.argmax(np.abs(cc)) - max_shift\n",
    "\n",
    "    tau = shift/float(interp*fs )\n",
    "\n",
    "    return tau , cc\n",
    "\n",
    "def myinwhole ( Dat ,fs , mics = 6) :\n",
    "    # Function that generates the GCC - PHAT function for all different microphone pair combinations\n",
    "\n",
    "    # Dat = input audio data ( data must be of shape mics X any audio length )\n",
    "    #fs is the sampling rate\n",
    "    #mics is the number of microphone (defaults in 6 for the respeaker)\n",
    "    ind = list(itertools.combinations(range(mics),2))\n",
    "    GCC = []\n",
    "\n",
    "    for i,j in ind:\n",
    "        t,c = gcc_phat(Dat[i], Dat[j], fs, 0.0003)\n",
    "\n",
    "    GCC.append(c)\n",
    "\n",
    "    GCC = np.array(GCC)\n",
    "    GCC = np.transpose(GCC)\n",
    "    # Change the 4 below with the interpolation factor given in gccphat () function\n",
    "    G = -len(c)/(2*4*fs)\n",
    "    #X and Y are included to be able to print the input image with matplotlib.pyplot.colormesh (X,Y, GCC )\n",
    "    Y = np.linspace(G,-G,len(c)+1)\n",
    "    X = np.linspace(1,len(ind),len(ind)+1)\n",
    "    return X , Y , GCC\n",
    "\n",
    "def myin(Dat,frame) :\n",
    "    # Function that generates the GCC - PHAT function for all different microphone pair combinations\n",
    "    \n",
    "    # Dat = input audio data ( data must be of shape mics X any audio length )\n",
    "    #fs is the sampling rate\n",
    "    #mics is the number of microphone (defaults in 6 for the respeaker)\n",
    "    ind = [[0,1],[0,2],[0,3],[0,4],[0,5],[1,2],[1,3],[1,4],[1,5],[2,3],[2,4],[2,5],[3,4],[3,5],[4,5]]\n",
    "    GCC = []\n",
    "\n",
    "    for i,j in ind:\n",
    "        t,c = gcc_phat(Dat[i][frame], Dat[j][frame], fs, 0.0003)\n",
    "        GCC.append(c)\n",
    "\n",
    "    GCC = np.array(GCC)\n",
    "    GCC = np.transpose(GCC)\n",
    "    # Change the 4 below with the interpolation factor given in gccphat () function\n",
    "    G = -len(c)/(2*4*fs)\n",
    "    #X and Y are included to be able to print the input image with matplotlib.pyplot.colormesh (X,Y, GCC )\n",
    "    Y = np.linspace(G,-G,len(c)+1)\n",
    "    X = np.linspace(1,len(ind),len(ind)+1)\n",
    "    return X , Y , GCC\n",
    "\n",
    "def roomsimdata(rt60,dim ,fs,data,sloc,miccent,mich,deg,delay =1,snr = None , N =1):\n",
    "    \n",
    "    #Function that simulates a room and returns the signals captured by the microphones\n",
    "    \n",
    "    #rt60 is the time it takes for reverberations to fall below 60dB\n",
    "    #dim is a 3D array having the xyz dimensions of the room\n",
    "    #fs is the sampling frequency\n",
    "    #data is the length N array of the wav data of the source\n",
    "    #sloc is a Nx3 array of the source location in xyz\n",
    "    #delay is how much to wait before start playing the source\n",
    "    #miccent is a 2D array of the center of the mic array\n",
    "    #mich is the height at which the mic array is placed\n",
    "    #deg is an array with the degrees of each source\n",
    "    #snr is the SNR at the mic signals (default is set to None)\n",
    "    #N is the number of sources\n",
    "    \n",
    "    \n",
    "    e_absorption, max_order = pra.inverse_sabine(rt60, dim)\n",
    "    # creates room object\n",
    "    room = pra.ShoeBox(\n",
    "        dim, fs, materials=pra.Material(e_absorption), max_order=max_order\n",
    "    )\n",
    "\n",
    "    \n",
    "    #adds sources to the room\n",
    "    for i in range(N):\n",
    "        room.add_source(sloc[:,deg[i]], signal= data[i], delay = delay)\n",
    "    \n",
    "    #generates mic lcoations\n",
    "    G = pra.beamforming.circular_2D_array( miccent , 6 , 0 , 0.0463 )\n",
    "    mic = []\n",
    "\n",
    "\n",
    "    for i in range(0,6):\n",
    "        c = [ G[0][i],G[1][i], mich]\n",
    "        mic.append(c) \n",
    "\n",
    "    mic_locs = np.c_[mic[0],mic[1],mic[2],mic[3],mic[4],mic[5]]   \n",
    "    #adds the microphone locations in the room\n",
    "    room.add_microphone_array(mic_locs)\n",
    "    \n",
    "    # finds the room impulse responses\n",
    "    room.compute_rir()\n",
    "    #r = room.rir[1][0]\n",
    "    #plt.plot(np.arange(len(r)) / room.fs, r)\n",
    "    #plt.title(\"The RIR from source to mic 1\")\n",
    "    #plt.xlabel(\"Time /s\")\n",
    "    \n",
    "    #simulates the room for a given SNR\n",
    "    room.simulate(snr)\n",
    "    \n",
    "    #returns the signals captured by the microphone array\n",
    "    return room.mic_array.signals\n",
    "\n",
    "def myvad(data ,sr ,time, th):\n",
    "    #This function performs voice activity of detection with an energy threshold\n",
    "    \n",
    "    # data (numpy array) to be classified as voiced / unvoiced\n",
    "    #sr is the sampling rate\n",
    "    # time is the time of the frames\n",
    "    #th is the energy threshold above which a frame is considered voiced\n",
    "    \n",
    "    Z = [stftprep(data[i], sr, time/1000) for i in range(6)]\n",
    "    ind = [i for i in range(len(Z[0])) if np.sum(np.square(Z[0][i]))/len(Z[0]) > th and np.sum(np.square(Z[1][i]))/len(Z[0]) > th and np.sum(np.square(Z[2][i]))/len(Z[0]) > th and np.sum(np.square(Z[3][i]))/len(Z[0]) > th and np.sum(np.square(Z[4][i]))/len(Z[0]) > th and np.sum(np.square(Z[5][i]))/len(Z[0]) > th]\n",
    "    Roo2 = []\n",
    "    for l in range(len(Z)):\n",
    "        emin = [Z[l][k] for k in ind]\n",
    "        eminor = np.array([j for i in emin for j in i])\n",
    "        Roo2.append(eminor)\n",
    "    return Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac04a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load speech data from AMI corpus\n",
    "l ,fs = librosa.load(\"ed1a.wav\", sr = 16000)\n",
    "z ,fs = librosa.load(\"ed1c.wav\", sr = 16000)\n",
    "p ,fs = librosa.load(\"ed1d.wav\", sr = 16000)\n",
    "\n",
    "#do VAD and seperate them in 2 second utterances\n",
    "time = 2\n",
    "L = VAD(3,l,fs, 30)\n",
    "L = stftprep(L,fs, time)\n",
    "\n",
    "Z = VAD(3,z,fs, 30)\n",
    "Z = stftprep(Z,fs, time)\n",
    "\n",
    "P = VAD(3,p,fs, 30)\n",
    "P = stftprep(P,fs, time)\n",
    "\n",
    "G = np.concatenate((L,P,Z))\n",
    "np.shape(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0o777\n",
    "os.mkdir('GCC',mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part simulates a room for all of the specified degrees and obtains the GCCs \n",
    "#for different utterances to be used in learning\n",
    "\n",
    "time = 150 #frame length in ms\n",
    "threshold = 5.0149176589231265e-05 \n",
    "\n",
    "\n",
    "#center of source (same as for microphone for this simulation)\n",
    "source_center = [2,2]\n",
    "#distance of source from microphone array\n",
    "radius = 1.5\n",
    "\n",
    "source_height = 1.5\n",
    "source = pra.beamforming.circular_2D_array(source_center,360,0,radius)\n",
    "#source = np.hstack((source[:,330:360],source[:,0:30])) #for 60 degrees\n",
    "source = np.vstack([source, np.ones(360)*source_height])\n",
    "rt60 = 0.4 #reverberation time\n",
    "r_dimensions = [5,5,3] #room dimensions xyz\n",
    "mic_center = [2,2] #microphone center\n",
    "degrees = np.arange(0,360, 10) # degrees\n",
    "mic_height = 1.5 # micorphone height\n",
    "delay = 0.1 # delay after which source plays\n",
    "snr = [None] #snrs\n",
    "Nosources = 1 # have to define the numebr of sources\n",
    "fs = 16000 #sampling rate\n",
    "path = r\"\" #path where the training data is to be saved\n",
    "\n",
    "for l in range(len(degrees)):\n",
    "    df = pd.DataFrame()\n",
    "    for j in range(len(snr)):\n",
    "        for k in range(150):\n",
    "            Room = roomsimdata(rt60,r_dimensions ,fs,[G[k]],source,mic_center,mic_height,[degrees[l]],delay,snr[j],Nosources) #change L,Z,P for different voice sets\n",
    "            #Room = [Room[i] for i in range(6)]\n",
    "            Voiced = myvad(Room,fs, time ,threshold)\n",
    "            Voiced = [stftprep(Voiced[i],fs,time/1000) for i in range(6)]\n",
    "            for m in range(len(Voiced)):\n",
    "                X,Y,GCC = myin(Voiced,m)\n",
    "                #plt.figure()\n",
    "                #plt.pcolormesh(X,Y, np.abs(GCC), vmin=0, vmax= np.max(abs(GCC)))\n",
    "                #plt.show()\n",
    "                print(f'degree{l},snr {j}, word {k}')\n",
    "                df = df.append(pd.DataFrame(GCC))\n",
    "    df.to_csv(path+'GCC/gcc' + str(degrees[l])+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code simulates a room, plots the GCC of an utterance and plays the simulated sound received by a microphone\n",
    "#to check if the simulation is representative of the real data\n",
    "\n",
    "#center of source (same as for microphone for this simulation)\n",
    "cent = [2,2]\n",
    "#distance of source from microphone array\n",
    "rad = [1.5,3,5]\n",
    "\n",
    "#center of source (same as for microphone for this simulation)\n",
    "source_center = [2,2]\n",
    "#distance of source from microphone array\n",
    "radius = 1.5\n",
    "\n",
    "source_height = 1.5\n",
    "source = pra.beamforming.circular_2D_array(source_center,360,0,radius)\n",
    "#source = np.hstack((source[:,330:360],source[:,0:30])) #for 60 degrees\n",
    "source = np.vstack([source, np.ones(360)*source_heigth])\n",
    "rt60 = 0.4 #reverberation time\n",
    "r_dimensions = [5,5,3] #room dimensions xyz\n",
    "mic_center = [2,2] #microphone center\n",
    "degrees = np.arange(0,360, 10) # degrees\n",
    "mic_height = 1.5 # micorphone height\n",
    "delay = 0.1 # delay after which source plays\n",
    "snr = [None] #snrs\n",
    "Nosources = 1 # have to define the numebr of sources\n",
    "fs = 16000 #sampling rate\n",
    "Roo = roomsimdata(rt60,dim ,fs,[G[1007]],source,miccent,mich,[degrees[0]],delay,snr[0],Nosources)\n",
    "\n",
    "Dat = [Roo[i] for i in range(6)]\n",
    "X,Y,GCC = myinwhole(Dat,fs)\n",
    "plt.figure()\n",
    "plt.pcolormesh(X,Y, np.abs(GCC), vmin=0, vmax= np.max(abs(GCC)))\n",
    "plt.title('GCC-PHAT Input Matrix')\n",
    "plt.xlabel('Mircophone Pairs')\n",
    "plt.ylabel('Time Delay /s')\n",
    "plt.show()\n",
    "\n",
    "sd.play(Roo[0], fs)\n",
    "status = sd.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
