{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows the available processing devices that can be used by tensorflow as well as the one currently used \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataread(path,rotangles):\n",
    "    #Function that reads the .csv files in the GCC folder created by the respeaker and\n",
    "    #stores all the gcc data in a numpy array for further processing\n",
    "    \n",
    "    #path is the path where the GCC folder is stored\n",
    "    #rotangles are the recorded angles extracted by the readme.txt\n",
    "    c = []\n",
    "    \n",
    "    for i in range(len(rotangles)):\n",
    "        #.csv file is read into a pandas dataframe\n",
    "        df = pd.read_csv(path + 'GCC/gcc' + str(int(rotangles[i]))+'.csv')\n",
    "        #light preprocessign that removes None values\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.drop(labels='Unnamed: 0', axis=1)\n",
    "        print(f\"Processing degree no {rotangles[i]}.\")\n",
    "        c.append(df)\n",
    "    Tensors = np.array([c[i].to_numpy() for i in range(len(rotangles))])\n",
    "    return Tensors\n",
    "\n",
    "def datseperate(Tensors,samples,ptr,pte, N =39):\n",
    "    #this function seperates the obtained data into training,tests and validation for each degree\n",
    "    \n",
    "    #Tensors is the numpy array that has all the GCCs for each degree\n",
    "    #samples is the number of GCCs for every degree\n",
    "    #ptr is the percentage of training data\n",
    "    #pte is the percentage of test data \n",
    "    #the rest data are validation data\n",
    "    #size of GCC (default is 39 for interpolation of 4)\n",
    "    \n",
    "    \n",
    "    #sets the indexes for the splitting of training,test and validation \n",
    "    pertr = int(ptr * samples)\n",
    "    perte = int(pte* samples+pertr)\n",
    "    \n",
    "    inputs_train = []\n",
    "    inputs_test = []\n",
    "    inputs_validate = []\n",
    "    \n",
    "\n",
    "    for k in range(len(Tensors)):  \n",
    "        datconst = Tensors[k][int(samples*N*(0)):int(samples*N*(1)),:]\n",
    "        a = np.array(np.split(datconst,samples,axis =0))\n",
    "        itr , ite ,iva = np.split(a, [pertr, perte])\n",
    "        inputs_train.append(itr)\n",
    "        inputs_test.append(ite)\n",
    "        inputs_validate.append(iva)\n",
    "    return inputs_train,inputs_test,inputs_validate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code gets the required metadata by the readme.txt\n",
    "\n",
    "#path\n",
    "path = \"\"\n",
    "\n",
    "#Opens the Readme.txt file and reads the metadata in the form written in the Gui.py\n",
    "f = open(path + 'Readme.txt' , 'r')\n",
    "cont = f.readlines()\n",
    "cont = [float(cont[i].split('is ')[1].split('\\n')[0]) for i in range(len(cont))]\n",
    "fs,duration,start,stop,res,samples = cont\n",
    "f.close()\n",
    "\n",
    "rotangles = np.arange(start,stop+res,res)\n",
    "\n",
    "#Reads the GCC from the .csv files\n",
    "Tensors = dataread(path,rotangles)\n",
    "\n",
    "\n",
    "N = 39\n",
    "percent_training = 0.6\n",
    "percent_test = 0.2\n",
    "\n",
    "training, test, validation =datseperate(Tensors,samples,percent_training,percent_test,N)\n",
    "\n",
    "#The following 3 lines can be uncommented to concatenate the training, test and validation data of multiple recordings\n",
    "#training = np.concatenate((training1,training2,training3),axis = 2)\n",
    "#test = np.concatenate((test1,test2,test3),axis = 2)\n",
    "#validation = np.concatenate((validation1,validation2,validation3),axis = 2)\n",
    "\n",
    "training = np.reshape(training,(len(training)*len(training[0]),N,15))\n",
    "test = np.reshape(test,(len(test)*len(test[0]),N,15))\n",
    "validation = np.reshape(validation,(len(validation)*len(validation[0]),N,15))\n",
    "\n",
    "#Creates one hot encoded labels\n",
    "ohg = np.eye(len(rotangles))\n",
    "training_labels = np.repeat(ohg,int(len(training)/len(rotangles)),axis =0)\n",
    "test_labels = np.repeat(ohg,int(len(test)/len(rotangles)),axis =0)\n",
    "validation_labels = np.repeat(ohg,int(len(validation)/len(rotangles)),axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code sets the model callbacks and the paths to save the model and the tensorboard information\n",
    "\n",
    "tbpath = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "mdpath = \"logs/train/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +'model.{epoch:02d}.h5'\n",
    "    \n",
    "tb_callback = tf.keras.callbacks.TensorBoard(tbpath,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=2\n",
    ")\n",
    "\n",
    "\n",
    "model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    mdpath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only= True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None\n",
    ")\n",
    "\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    min_delta=0,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode=\"max\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b30735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part preprocesses the data to be inserted in the CNN, creates the LENET5 model and trains it\n",
    "\n",
    "#This is done because Convolutional Neural Networks in tensorflow require one extra dimension\n",
    "training = np.expand_dims(training,3)\n",
    "test = np.expand_dims(test,3)\n",
    "validation = np.expand_dims(validation,3)\n",
    "\n",
    "lenet_5_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=training[0].shape, padding='same'), #C1\n",
    "    keras.layers.AveragePooling2D(), #S2\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C3\n",
    "    keras.layers.AveragePooling2D(), #S4\n",
    "    keras.layers.Flatten(), #Flatten\n",
    "    keras.layers.Dense(120, activation='tanh'), #C5\n",
    "    keras.layers.Dense(84, activation='tanh'), #F6\n",
    "    keras.layers.Dense(len(training_labels[0]), activation='softmax') #Output layer\n",
    "])\n",
    "\n",
    "lenet_5_model.compile(optimizer='adam', loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "history = lenet_5_model.fit(training, training_labels, epochs=100, batch_size=32,shuffle = True, validation_data=(validation, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4091eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part creates and trains the Multilayer Perceptron model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(N,15)))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(36, activation = 'softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.optimizer.lr.assign(0.00005)\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(training, training_labels, epochs=100, batch_size=32,shuffle = True, validation_data=(validation, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d581201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part evaluates the models and prints the predictions and the labels to get a better insight on the performance of the models\n",
    "#lenet_5_model.evaluate(test, test_labels,verbose=1)\n",
    "model.evaluate(test, test_labels,verbose=1)\n",
    "\n",
    "# use the model to predict the test inputs\n",
    "#predictions = lenet_5_model.predict(test)\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# print the predictions and the expected ouputs\n",
    "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
    "print(\"actual =\\n\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb01c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part saves the model in .h5 format, converts it in .tflite and prints its size \n",
    "\n",
    "#tf.saved_model.save(lenet_5_model,'.h5')\n",
    "tf.saved_model.save(model,'Model.h5')\n",
    "\n",
    "# Convert the model\n",
    "saved_model_dir = \"Model.h5\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"Model.tflite\", \"wb\").write(tflite_model)\n",
    "basic_model_size = os.path.getsize(\"Model.tflite\")\n",
    "print(\"Model is %d bytes\" % basic_model_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
